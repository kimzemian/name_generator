{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81b39041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import re\n",
    "\n",
    "\n",
    "#this copy uses padding to batch the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12ab026a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      " michael\n",
      "\n",
      "michael\n",
      "s\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/p7/dy_mpsj13tv174tw914k9y3m0000gn/T/ipykernel_55933/298148711.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"s\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# splitting data into names\n",
    "data = open('babynames.txt', 'r').read()\n",
    "data = data.lower()\n",
    "chars = list(set(data))\n",
    "vocab_size = len(chars)\n",
    "pattern = r'[^ \\n][^ \\n]*\\n'\n",
    "target = re.findall(pattern, data)\n",
    "pattern = r'[^\\n][^ \\n]+\\n'\n",
    "data = re.findall(pattern, data)\n",
    "maxlen = len(max(data, key=len))\n",
    "\n",
    "# Padding\n",
    "for i in range(len(data)):\n",
    "    print(i)\n",
    "    print(data[i])\n",
    "    print(target[i]+\"s\")\n",
    "    assert(len(data[i])==len(target[i]))\n",
    "    while len(data[i])<maxlen:\n",
    "        data[i] += '\\n'\n",
    "        target[i] += '\\n'\n",
    "\n",
    "# Creating a dictionary that maps integers to the characters\n",
    "int2char = dict(enumerate(chars))\n",
    "\n",
    "# Creating another dictionary that maps characters to integers\n",
    "char2int = {char: ind for ind, char in int2char.items()}\n",
    "\n",
    "\n",
    "# Reshaping data\n",
    "data = [[char2int[char] for char in name] for name in data]\n",
    "target = [[char2int[char] for char in name] for name in target]\n",
    "data = torch.tensor(data)\n",
    "target = torch.tensor(target)\n",
    "data = F.one_hot(data, num_classes=vocab_size).float()\n",
    "target = F.one_hot(target, num_classes=vocab_size).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae92b581",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        #Defining the layers\n",
    "        self.rnn = nn.RNN(input_size, hidden_dim)   \n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "    \n",
    "    def forward(self, input_seq, hidden):\n",
    "        out, hidden = self.rnn(input_seq, hidden)    #input_seq=seq_len*h_in, out=seq_len*h_out\n",
    "        out = self.fc(out)    #out=seq_len*output_size\n",
    "        \n",
    "        return out, hidden\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1b28af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "lr=0.01\n",
    "\n",
    "# Instantiate the model with hyperparameters\n",
    "model = Model(input_size=vocab_size, output_size=vocab_size, hidden_dim=100)\n",
    "\n",
    "# Define Loss, Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfcbd988",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(out, hidden, size):\n",
    "    print(out.shape)\n",
    "    name=torch.zeros(size)\n",
    "    for counter in range(size):\n",
    "        #get a probability distribution from output\n",
    "        prob = F.softmax(out, dim=1)\n",
    "        \n",
    "        #given prob generate a character randomly\n",
    "        index = np.random.choice(vocab_size, prob)\n",
    "        input_seq = F.one_hot(index, num_classes=vocab_size)\n",
    "    \n",
    "        name[count] = input_seq\n",
    "        out, hidden = model(input_seq , hidden)        \n",
    "    \n",
    "    #reverse one hot encoding\n",
    "    name = name.argmax(-1)\n",
    "    \n",
    "    #join characters\n",
    "    name = ''.join(int2char[char] for char in name)    \n",
    "    return name\n",
    "\n",
    "def sample(size):\n",
    "    name = torch.zeros(size, vocab_size)\n",
    "    index = random.randint(0,len(data)-1)\n",
    "    char = data[index]\n",
    "    \n",
    "    for counter in range(size):\n",
    "        name[counter] = char\n",
    "        char = char.view(1, vocab_size)\n",
    "        out, hidden = model(char)\n",
    "        \n",
    "        #get a probability distribution from output\n",
    "        prob = F.softmax(out, dim=0)\n",
    "        \n",
    "        #given prob generate a character randomly\n",
    "        char = torch.multinomial(prob, num_samples=1)\n",
    "        char = F.one_hot(char, num_classes=vocab_size).float()\n",
    "            \n",
    "    \n",
    "    #reverse one hot encoding\n",
    "    name = name.argmax(-1)\n",
    "    name = name.numpy()\n",
    "    #join characters\n",
    "    sample = ''.join(int2char[integer] for integer in name)    \n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0775fbbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1............. "
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'hidden_dim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/p7/dy_mpsj13tv174tw914k9y3m0000gn/T/ipykernel_55933/2783169378.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mepoch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch: {}.............'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcounter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_seq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Clears existing gradients from previous epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hidden_dim' is not defined"
     ]
    }
   ],
   "source": [
    "#Training run\n",
    "epoch=0\n",
    "while True:  #epochs\n",
    "    epoch += 1\n",
    "    print('Epoch: {}.............'.format(epoch), end=' ')\n",
    "    hidden = torch.zeros(1, hidden_dim)   \n",
    "    for counter, input_seq in enumerate(data):\n",
    "        optimizer.zero_grad() # Clears existing gradients from previous epoch\n",
    "        hidden = hidden.detach() #Prevents backpropagation through the hidden state\n",
    "        out, hidden = model(input_seq, hidden)\n",
    "        loss = criterion(output, target[counter])\n",
    "        loss.backward() # Does backpropagation and calculates gradients\n",
    "        optimizer.step() # Updates the weights accordingly\n",
    "    \n",
    "        # sample from the model now and then\n",
    "        if counter % 4 == 0:\n",
    "            name = sample(out[-1], hidden, 200)            \n",
    "            print('----\\n %s \\n----' % (name, ))\n",
    "            print(\"Loss: {:.4f}\".format(loss.item()))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b082d9cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
